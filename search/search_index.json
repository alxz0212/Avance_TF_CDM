{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83d\udce1 An\u00e1lisis Computacional de Geopol\u00edtica: El \"Gran Juego\" Post-Sovi\u00e9tico","text":"<p>Un enfoque de Big Data para entender la econom\u00eda y seguridad en Asia Central.</p> \ud83d\udc64 Daniel Alexis Mendoza Corne <p> Ingenier\u00eda Inform\u00e1tica y de Sistemas |          </p> <p> </p>"},{"location":"#resumen-ejecutivo","title":"\ud83d\udccc Resumen Ejecutivo","text":"<p>Este proyecto aplica t\u00e9cnicas de Big Data e Ingenier\u00eda de Software para resolver una pregunta fundamental de las Ciencias Pol\u00edticas:</p> <p>\"\u00bfSon los factores de 'Poder Duro' (Gasto Militar) o de 'Poder Blando' (Democracia, Control de Corrupci\u00f3n) los que determinan el desarrollo econ\u00f3mico en la periferia post-sovi\u00e9tica?\"</p> <p>A trav\u00e9s de un pipeline automatizado, se procesaron d\u00e9cadas de datos hist\u00f3ricos de pa\u00edses clave del \"Gran Juego\" (Afganist\u00e1n, Mongolia, C\u00e1ucaso) para modelar matem\u00e1ticamente sus trayectorias de desarrollo.</p>"},{"location":"#tech-stack","title":"\ud83d\udee0\ufe0f Tech Stack\ud83c\udfa5 Demo: El Gran Juego en Acci\u00f3n","text":"<p>Tablero interactivo con Globo 3D, An\u00e1lisis Comparativo y Simulador de IA.</p>"},{"location":"#estructura-de-navegacion","title":"\ud83d\uddfa\ufe0f Estructura de Navegaci\u00f3n","text":"\ud83e\udded Secci\u00f3n \ud83d\udcdd Descripci\u00f3n \ud83d\ude80 Gu\u00eda de Trabajo Paso a paso para completar el proyecto. Instrucciones detalladas. \ud83d\udc33 Infraestructura Explicaci\u00f3n t\u00e9cnica de Docker, servicios y redes. \ud83d\udcbb Cat\u00e1logo de C\u00f3digo Documentaci\u00f3n t\u00e9cnica de scripts Python (<code>src/</code>) y Pipeline ETL. \ud83d\udcca Resultados Informe final con gr\u00e1ficos, modelos y hallazgos del \"Gran Juego\"."},{"location":"#arquitectura-del-sistema","title":"\ud83c\udfd7\ufe0f Arquitectura del Sistema","text":"<p>El proyecto implementa un flujo de datos moderno y contenerizado:</p> <pre><code>graph TD\n    %% Estilos\n    classDef source fill:#f9f,stroke:#333,stroke-width:2px;\n    classDef script fill:#bbf,stroke:#333,stroke-width:2px,color:black;\n    classDef data fill:#dfd,stroke:#333,stroke-width:2px,color:black;\n    classDef output fill:#fd9,stroke:#333,stroke-width:2px,color:black,stroke-dasharray: 5 5;\n\n    subgraph INGESTA [\"\ud83d\udce1 Ingesta de Datos\"]\n        A[\"\u2601\ufe0f Internet / Repo QoG\"]:::source\n        Script1{{\"\ud83d\udc0d download_data.py\"}}:::script\n    end\n\n    subgraph PROCESAMIENTO [\"\u2699\ufe0f Procesamiento &amp; An\u00e1lisis\"]\n        Script2{{\"\u26a1 pipeline.py\"}}:::script\n        Script3{{\"\ud83e\udde0 analysis.py\"}}:::script\n        Script5{{\"\ud83d\udcc9 econometric_analysis.py\"}}:::script\n    end\n\n    subgraph ALMACENAMIENTO [\"\ud83d\udcbe Almacenamiento\"]\n        B[(\"\ud83d\udcc4 Raw CSV\")]:::data\n        C[(\"\ud83d\udce6 Clean Parquet\")]:::data\n    end\n\n    subgraph VISUALIZACION [\"\ud83d\udcca Consumo &amp; UI\"]\n        Script4{{\"\ud83d\ude80 app_streamlit.py\"}}:::script\n        D[\"\ud83d\udcc8 Gr\u00e1ficos Est\u00e1ticos .png\"]:::output\n        E[\"\ud83d\udda5\ufe0f Dashboard Interactivo\"]:::output\n        F[\"\ud83d\udcc4 Reporte Hausman .txt\"]:::output\n    end\n\n    %% Relaciones\n    A --&gt; Script1\n    Script1 --&gt; B\n    B --&gt; Script2\n    Script2 --&gt; C\n    C --&gt; Script3\n    C --&gt; Script4\n    C --&gt; Script5\n    Script3 --&gt; D\n    Script4 --&gt; E\n    Script5 --&gt; F</code></pre> <ul> <li>Infraestructura: Docker Compose orquestando JupyterLab, Spark Master/Worker.</li> <li>ETL: PySpark para limpieza y transformaci\u00f3n (<code>.parquet</code>).</li> <li>Anal\u00edtica:<ul> <li>Machine Learning: Random Forest (Spark MLlib) para Feature Importance.</li> <li>Econometr\u00eda: Modelos de Datos de Panel (Fixed Effects vs Random Effects) y Test de Hausman.</li> </ul> </li> <li>Frontend: Dashboard interactivo en Streamlit para exploraci\u00f3n de datos.</li> </ul>"},{"location":"#hallazgos-principales","title":"\ud83d\udd0d Hallazgos Principales","text":""},{"location":"#1-inteligencia-artificial-random-forest","title":"1. Inteligencia Artificial (Random Forest)","text":"<p>El modelo identific\u00f3 que, descontando la salud b\u00e1sica (<code>Esperanza de Vida</code>), los factores de Seguridad y Estabilidad del R\u00e9gimen tienen un peso predictivo superior a la mera democratizaci\u00f3n.</p>"},{"location":"#2-validacion-econometrica-test-de-hausman","title":"2. Validaci\u00f3n Econom\u00e9trica (Test de Hausman)","text":"<p>Se aplic\u00f3 un Test de Hausman comparando modelos de Efectos Fijos vs Aleatorios. - Resultado: Se prefiri\u00f3 el modelo de Efectos Fijos (\\(P &lt; 0.05\\)). - Interpretaci\u00f3n: Las caracter\u00edsticas \u00fanicas e invariables de cada pa\u00eds (\"El estilo uzbeko\", \"La geograf\u00eda afgana\") son determinantes estructurales del \u00e9xito o fracaso econ\u00f3mico, confirmando la hip\u00f3tesis de heterogeneidad regional.</p>"},{"location":"#instrucciones-de-despliegue","title":"\ud83d\ude80 Instrucciones de Despliegue","text":"<pre><code># 1. Levantar la infraestructura\ndocker compose up -d\n\n# 2. Descargar e Ingestar Datos\ndocker exec jupyter_lab python /home/jovyan/work/src/download_data.py\n\n# 3. Ejecutar Pipeline ETL (Raw -&gt; Parquet)\ndocker exec jupyter_lab python /home/jovyan/work/src/pipeline.py\n\n# 4. Entrenar Modelo de Machine Learning (Spark)\ndocker exec jupyter_lab spark-submit /home/jovyan/work/src/analysis.py\n\n# 5. Ejecutar An\u00e1lisis Econom\u00e9trico (Hausman)\ndocker exec jupyter_lab python /home/jovyan/work/src/econometric_analysis.py\n\n# 6. Lanzar Dashboard Web (http://localhost:8501)\n# Opci\u00f3n A: Versi\u00f3n Cl\u00e1sica\ndocker exec -d jupyter_lab streamlit run /home/jovyan/work/src/app_streamlit.py\n\n# Opci\u00f3n B: Versi\u00f3n PRO (3D Command Center) \ud83c\udf1f\ndocker exec -d jupyter_lab streamlit run /home/jovyan/work/src/app_streamlit_pro.py\n</code></pre>"},{"location":"#estructura-del-repositorio","title":"\ud83d\udcc2 Estructura del Repositorio","text":"<pre><code>\u251c\u2500\u2500 01_README.md                # Portada del proyecto (Este archivo)\n\u251c\u2500\u2500 02_INFRAESTRUCTURA.md       # Documentaci\u00f3n t\u00e9cnica de Docker\n\u251c\u2500\u2500 03_RESULTADOS.md            # Informe detallado de hallazgos\n\u251c\u2500\u2500 04_REFLEXION_IA.md          # Bit\u00e1cora de co-creaci\u00f3n con IA\n\u251c\u2500\u2500 05_EXPLICACION_CODIGO.md    # Cat\u00e1logo de scripts\n\u251c\u2500\u2500 06_RESPUESTAS.md            # Preguntas de defensa\n\u251c\u2500\u2500 docker-compose.yml          # Orquestaci\u00f3n\n\u251c\u2500\u2500 src/                        # C\u00f3digo Fuente Python\n\u2502   \u251c\u2500\u2500 pipeline.py             # L\u00f3gica ETL Big Data\n\u2502   \u251c\u2500\u2500 analysis.py             # ML Engine\n\u2502   \u251c\u2500\u2500 econometric_analysis.py # Stats Engine\n\u2502   \u2514\u2500\u2500 app_streamlit.py        # Web App\n\u2514\u2500\u2500 data/                       # Lakehouse (Raw + Processed)\n</code></pre> <p>\u00daltima actualizaci\u00f3n correcci\u00f3n visual: v3.0 (Markdown Table)</p>"},{"location":"01_README/","title":"Trabajo Final: El \"Gran Juego\" Post-Sovi\u00e9tico","text":"<p>Alumno: Daniel Alexis Mendoza Corne Fecha: Febrero 2026</p>"},{"location":"01_README/#demostracion","title":"\ud83c\udfa5 Demostraci\u00f3n","text":"<p>\u00bfQuieres ver c\u00f3mo qued\u00f3 el Dashboard? \ud83d\udc49 Mira el video aqui</p>"},{"location":"01_README/#orden-de-trabajo","title":"Orden de trabajo","text":"<p>Completa los archivos en este orden. Cada numero indica la secuencia:</p> Orden Archivo Que haces 1 <code>01_README.md</code> (este archivo) Defines tu pregunta, paises y variables 2 <code>02_INFRAESTRUCTURA.md</code> Construyes y explicas tu docker-compose.yml 3 <code>src/verify_spark.py</code> Verificas la conexi\u00f3n con Spark 4 <code>src/pipeline.py</code> ETL: Limpieza y Transformaci\u00f3n en Parquet 5 <code>src/analysis.py</code> An\u00e1lisis con ML (Random Forest) en Spark 6 <code>src/econometric_analysis.py</code> An\u00e1lisis Econom\u00e9trico (Test de Hausman) 7 <code>03_RESULTADOS.md</code> Presentas graficos e interpretas resultados 8 <code>04_REFLEXION_IA.md</code> Documentas tu proceso y pegas tus prompts 9 <code>05_EXPLICACION_CODIGO.md</code> Cat\u00e1logo t\u00e9cnico de todos los scripts 10 <code>06_RESPUESTAS.md</code> Respondes 4 preguntas de comprension 11 <code>07_PROTOTIPO.md</code> Nuevo: Video Demo del Dashboard <p>Los archivos <code>docker-compose.yml</code>, <code>requirements.txt</code> y <code>.gitignore</code> los completas conforme avanzas.</p>"},{"location":"01_README/#pregunta-de-investigacion","title":"Pregunta de investigacion","text":"<p>\"\u00bfQu\u00e9 influye m\u00e1s en la riqueza de los pa\u00edses ex-sovi\u00e9ticos: tener un ej\u00e9rcito fuerte y gastar mucho en armas, o ser un pa\u00eds m\u00e1s democr\u00e1tico y con menos corrupci\u00f3n?\"</p>"},{"location":"01_README/#paises-seleccionados-5","title":"Paises seleccionados (5)","text":"# Pais Codigo ISO Por que lo elegiste 1 Afghanistan AFG Actor central hist\u00f3rico y geopol\u00edtico en la regi\u00f3n (\"Cementerio de Imperios\"). 2 Mongolia MNG Estado tap\u00f3n estrat\u00e9gico y neutral entre las potencias Rusia y China. 3 Azerbaijan AZE Pieza clave en la conexi\u00f3n Caspio-C\u00e1ucaso y seguridad energ\u00e9tica. 4 Georgia GEO Referente de aspiraciones democr\u00e1ticas y occidentales en la regi\u00f3n. 5 Armenia ARM Aliado estrat\u00e9gico tradicional de Rusia en el C\u00e1ucaso Sur. <p>IMPORTANTE: No puedes usar los paises del ejemplo del profesor (KAZ, UZB, TKM, KGZ, TJK).</p>"},{"location":"01_README/#variables-seleccionadas-5-numericas","title":"Variables seleccionadas (5 numericas)","text":"# Variable QoG Que mide Por que la elegiste 1 <code>gle_cgdpc</code> PIB per c\u00e1pita real Variable Objetivo (Target): Indicador est\u00e1ndar de desarrollo econ\u00f3mico. 2 <code>wdi_expmil</code> Gasto militar (% PIB) Poder Duro: Refleja la priorizaci\u00f3n de seguridad sobre bienestar. 3 <code>p_polity2</code> \u00cdndice de Democracia Poder Blando: Mide la estabilidad y apertura del r\u00e9gimen pol\u00edtico. 4 <code>vdem_corr</code> \u00cdndice de Corrupci\u00f3n Calidad Institucional: Factor clave que afecta la inversi\u00f3n y crecimiento. 5 <code>wdi_lifexp</code> Esperanza de vida Variable de Control: Indicador b\u00e1sico de bienestar social y salud. <p>Tip: Consulta el codebook de QoG para entender que mide cada variable: https://www.gu.se/en/quality-government/qog-data</p>"},{"location":"01_README/#variable-derivada","title":"Variable derivada","text":"<p>He creado la variable <code>subregion</code> para agrupar geogr\u00e1ficamente a los pa\u00edses y capturar din\u00e1micas regionales distintas m\u00e1s all\u00e1 de las fronteras nacionales:</p> <ul> <li>Caucasus: Azerbaiy\u00e1n, Georgia, Armenia.</li> <li>Central/South: Afganist\u00e1n.</li> <li>East: Mongolia.</li> </ul>"},{"location":"01_README/#tipo-de-analisis-elegido","title":"Tipo de analisis elegido","text":"<ul> <li> Clustering (K-Means)</li> <li> Serie temporal (evolucion por pais)</li> <li> Comparacion (Regresi\u00f3n Random Forest - Importancia de Factores)</li> <li> Modelo Econom\u00e9trico (Panel Data - Efectos Fijos vs Aleatorios)</li> </ul>"},{"location":"01_README/#como-ejecutar-mi-pipeline","title":"Como ejecutar mi pipeline","text":"<pre><code># Paso 1: Levantar infraestructura\ndocker compose up -d\n\n# Paso 2: Verificar que todo funciona\ndocker ps\n\n# Paso 3: Descargar Datos (Autom\u00e1tico)\ndocker exec jupyter_lab python /home/jovyan/work/src/download_data.py\n\n# Paso 4: Ejecutar pipeline ETL (Procesamiento de Datos)\n# Nota: Ejecutar desde dentro del contenedor o tener Spark local.\n# Si usas Docker (recomendado):\ndocker exec jupyter_lab python /home/jovyan/work/src/pipeline.py\n\n# Paso 5: Ejecutar An\u00e1lisis y Generar Gr\u00e1ficos\ndocker exec jupyter_lab spark-submit /home/jovyan/work/src/analysis.py\n\n# Paso 6: Ejecutar An\u00e1lisis Econom\u00e9trico (Hausman)\n# Nota: Ejecutar desde entorno con librer\u00edas 'linearmodels' instaladas (puede ser local si tienes entorno)\ndocker exec jupyter_lab python /home/jovyan/work/src/econometric_analysis.py\n# (Si no corre en docker por falta de librer\u00edas, instalar: pip install linearmodels statsmodels)\n</code></pre> <p>El an\u00e1lisis generar\u00e1 los gr\u00e1ficos en la carpeta <code>notebooks/</code> y el reporte final est\u00e1 en <code>03_RESULTADOS.md</code>.</p>"},{"location":"01_README/#estructura-del-proyecto","title":"Estructura del Proyecto","text":"<pre><code>\u251c\u2500\u2500 01_README.md                # Este archivo\n\u251c\u2500\u2500 02_INFRAESTRUCTURA.md       # Documentaci\u00f3n de Docker y Servicios\n\u251c\u2500\u2500 03_RESULTADOS.md            # Informe final con gr\u00e1ficos e interpretaci\u00f3n\n\u251c\u2500\u2500 04_REFLEXION_IA.md          # Bit\u00e1cora de aprendizaje y Prompts\n\u251c\u2500\u2500 05_EXPLICACION_CODIGO.md    # Cat\u00e1logo y explicaci\u00f3n t\u00e9cnica de scripts\n\u251c\u2500\u2500 06_RESPUESTAS.md            # Preguntas de comprensi\u00f3n\n\u251c\u2500\u2500 07_PROTOTIPO.md             # Video Demo del proyecto (Prototipo)\n\u251c\u2500\u2500 INSTRUCCIONES_DESPLIEGUE.txt# Cheat Sheet con comandos para ejecutar\n\u251c\u2500\u2500 capturas/                   # Im\u00e1genes de evidencia\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 processed/              # Datos transformados (Parquet)\n\u2502   \u2514\u2500\u2500 raw/                    # Dataset original (CSV)\n\u251c\u2500\u2500 docker/\n\u2502   \u2514\u2500\u2500 Dockerfile              # Definici\u00f3n de la imagen Jupyter+Spark\n\u251c\u2500\u2500 docker-compose.yml          # Orquestaci\u00f3n de servicios\n\u251c\u2500\u2500 jars/                       # Drivers JDBC (Postgres)\n\u251c\u2500\u2500 notebooks/\n\u2502   \u251c\u2500\u2500 01_analisis_asia_central.ipynb # Notebook exploratorio\n\u2502   \u251c\u2500\u2500 02_analisis_gran_juego.ipynb   # Notebook principal del an\u00e1lisis\n\u2502   \u251c\u2500\u2500 grafico_correlacion.png\n\u2502   \u251c\u2500\u2500 grafico_feature_importance.png\n\u2502   \u2514\u2500\u2500 hausman_results.txt\n\u251c\u2500\u2500 requirements.txt            # Dependencias Python\n\u2514\u2500\u2500 src/\n    \u251c\u2500\u2500 analysis.py             # Script de an\u00e1lisis ML (Spark-Submit)\n    \u251c\u2500\u2500 app_streamlit.py        # Dashboard Interactivo Web\n    \u251c\u2500\u2500 download_data.py        # Script de descarga autom\u00e1tica\n    \u251c\u2500\u2500 econometric_analysis.py # Script econom\u00e9trico (Hausman)\n    \u251c\u2500\u2500 ingest_data.py          # Script de ingesti\u00f3n a Postgres (Legacy)\n    \u251c\u2500\u2500 pipeline.py             # Script ETL (Limpieza y Transformaci\u00f3n)\n    \u2514\u2500\u2500 verify_spark.py         # Test de conectividad Spark\n</code></pre>"},{"location":"01_README/#licencia","title":"\ud83d\udcc4 Licencia","text":"<p>Este proyecto est\u00e1 bajo la Licencia MIT. Consulta el archivo LICENSE para m\u00e1s detalles.</p> <p>Copyright (c) 2026 Alexis M.</p>"},{"location":"02_INFRAESTRUCTURA/","title":"Paso 2: Infraestructura Docker","text":"<ul> <li>Alumno: Daniel Alexis Mendoza Corne</li> <li>Fecha: Febrero 2026</li> </ul>"},{"location":"02_INFRAESTRUCTURA/#21-mi-docker-composeyml-explicado","title":"2.1 Mi docker-compose.yml explicado","text":""},{"location":"02_INFRAESTRUCTURA/#servicio-postgresql","title":"Servicio: PostgreSQL","text":"<pre><code>postgres:\n  image: postgres:13\n  container_name: postgres_db\n  environment:\n    POSTGRES_USER: user\n    POSTGRES_PASSWORD: password\n    POSTGRES_DB: qog_data\n  ports:\n    - \"5432:5432\"\n  volumes:\n    - ./data/postgres:/var/lib/postgresql/data\n  networks:\n    - bigdata-net\n</code></pre> <p>Que hace: Este servicio levanta una base de datos relacional PostgreSQL versi\u00f3n 13.</p> <ul> <li><code>container_name</code>: Le da un nombre fijo (<code>postgres_db</code>) para que otros contenedores (como Spark o Jupyter) puedan encontrarlo f\u00e1cilmente en la red interna usando ese nombre como si fuera un dominio DNS.</li> <li><code>environment</code>: Define las credenciales (usuario/contrase\u00f1a) y el nombre de la base de datos que se crear\u00e1 autom\u00e1ticamente al iniciar.</li> <li><code>ports</code>: Mapea el puerto 5432 del contenedor al 5432 de mi m\u00e1quina local (\"host\"), permiti\u00e9ndome conectarme desde fuera de Docker (por ejemplo, con DBeaver).</li> <li><code>volumes</code>: Crea una persistencia de datos. Si borro el contenedor, los datos guardados en <code>/var/lib/postgresql/data</code> (dentro del contenedor) se conservan en mi carpeta local <code>./data/postgres</code>.</li> <li><code>networks</code>: Conecta el servicio a una red compartida (<code>bigdata-net</code>) para hablar con los otros servicios.</li> </ul>"},{"location":"02_INFRAESTRUCTURA/#servicio-spark-master","title":"Servicio: Spark Master","text":"<pre><code>spark-master:\n  image: apache/spark:3.5.1\n  container_name: spark_master\n  hostname: spark-master\n  environment:\n    - SPARK_MODE=master\n    - SPARK_RPC_AUTHENTICATION_ENABLED=no\n    - SPARK_RPC_ENCRYPTION_ENABLED=no\n    - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\n    - SPARK_SSL_ENABLED=no\n  command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master\n  ports:\n    - \"8080:8080\" # Web UI\n    - \"7077:7077\" # Master URL\n  volumes:\n    - ./jars:/opt/spark/jars # Compartir drivers\n  networks:\n    - bigdata-net\n</code></pre> <p>Que hace: Es el nodo coordinador (\"cerebro\") del cluster Spark.</p> <ul> <li>Rol: Gestiona los recursos disponibles y asigna las tareas de procesamiento a los Workers. No procesa datos pesados \u00e9l mismo, solo orquesta.</li> <li>Puertos:</li> <li><code>8080</code>: Expone la interfaz web (Spark UI) donde puedo ver el estado del cluster, los workers conectados y los jobs en ejecuci\u00f3n.</li> <li><code>7077</code>: Es el puerto de comunicaci\u00f3n interna que usan los Workers y los Clientes (como Jupyter) para enviar trabajos al Master.</li> <li>Red: Necesita la red compartida para que el Worker pueda decir \"Hola, estoy aqu\u00ed\" enviando mensajes a <code>spark-master:7077</code>.</li> </ul>"},{"location":"02_INFRAESTRUCTURA/#servicio-spark-worker","title":"Servicio: Spark Worker","text":"<pre><code>spark-worker:\n  image: apache/spark:3.5.1\n  container_name: spark_worker\n  environment:\n    - SPARK_MODE=worker\n    - SPARK_WORKER_MEMORY=1G\n    - SPARK_WORKER_CORES=1\n    # ... (configuraciones de seguridad desactivadas)\n  command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077\n  depends_on:\n    - spark-master\n  volumes:\n    - ./jars:/opt/spark/jars # Compartir drivers\n  networks:\n    - bigdata-net\n</code></pre> <p>Que hace: Es el \"m\u00fasculo\" del cluster. Es quien realmente ejecuta los c\u00e1lculos.</p> <ul> <li>Conexi\u00f3n: En el comando de inicio (<code>command</code>) le especificamos expl\u00edcitamente <code>spark://spark-master:7077</code>. As\u00ed sabe a qui\u00e9n reportarse.</li> <li>Recursos: Le asign\u00e9 1 Core (<code>SPARK_WORKER_CORES=1</code>) y 1GB de RAM (<code>SPARK_WORKER_MEMORY=1G</code>). Si agrego m\u00e1s workers (escalado horizontal), tendr\u00eda m\u00e1s capacidad de procesamiento paralelo total.</li> <li>Drivers: Monta el volumen <code>./jars</code> igual que el master y jupyter para asegurarse de tener las mismas librer\u00edas (como el driver de PostgreSQL) disponibles.</li> </ul>"},{"location":"02_INFRAESTRUCTURA/#otros-servicios-jupyterlab","title":"Otros servicios: JupyterLab","text":"<pre><code>jupyter-lab:\n  image: jupyter/pyspark-notebook:latest\n  container_name: jupyter_lab\n  ports:\n    - \"8888:8888\" # JupyterLab\n    - \"4040:4040\" # Spark UI (Driver)\n  environment:\n    - JUPYTER_ENABLE_LAB=yes\n    - JUPYTER_TOKEN=bigdata2024 # Contrase\u00f1a fija\n    - SPARK_MASTER=spark://spark-master:7077\n    - SPARK_DRIVER_HOST=jupyter_lab\n  # ...\n</code></pre> <p>Que hace: Act\u00faa como mi \"Cliente\" o \"Driver\" interactivo. Es donde escribo el c\u00f3digo Python (PySpark).</p> <ul> <li>Se conecta al Master (<code>SPARK_MASTER</code>) para enviar el c\u00f3digo a ejecutar.</li> <li>Expone el puerto <code>4040</code> para ver el detalle de la ejecuci\u00f3n de mi aplicaci\u00f3n espec\u00edfica.</li> </ul>"},{"location":"02_INFRAESTRUCTURA/#acceso-y-credenciales","title":"Acceso y Credenciales","text":"<p>Para acceder a JupyterLab:</p> <ul> <li>URL: <code>http://localhost:8888</code></li> <li>Password/Token: <code>bigdata2024</code> (Configurado en <code>docker-compose.yml</code>)</li> </ul>"},{"location":"02_INFRAESTRUCTURA/#comandos-de-ejecucion-importante","title":"Comandos de Ejecuci\u00f3n (Importante)","text":"<p>Dado que nuestro entorno est\u00e1 en Docker, debemos ejecutar los scripts dentro del contenedor, no desde nuestro Windows local.</p> <p>1. Instalar dependencias nuevas:</p> <pre><code>docker exec -it jupyter_lab pip install -r /home/jovyan/work/requirements.txt\n</code></pre> <p>2. Ejecutar Pipeline ETL:</p> <pre><code>docker exec -it jupyter_lab spark-submit /home/jovyan/work/src/pipeline.py\n</code></pre>"},{"location":"02_INFRAESTRUCTURA/#22-healthchecks","title":"2.2 Healthchecks","text":"<p>\u00bfQu\u00e9 son? Un healthcheck es un comando que Docker ejecuta peri\u00f3dicamente dentro del contenedor para preguntar: \"\u00bfEst\u00e1s realmente listo para trabajar?\".</p> <p>\u00bfPor qu\u00e9 los necesitamos? Docker por defecto solo sabe si el contenedor \"arranc\u00f3\" (el proceso existe), pero no si la aplicaci\u00f3n est\u00e1 lista.</p> <ul> <li>Ejemplo: PostgreSQL puede tardar 10 segundos en iniciar y aceptar conexiones.</li> <li>Sin healthcheck: Si Spark o mi script de ingesta intenta conectarse en el segundo 2, fallar\u00e1 con un error de conexi\u00f3n, aunque Docker diga que la base de datos est\u00e1 \"running\".</li> <li>Con healthcheck: Podr\u00edamos configurar los servicios dependientes para que esperen (<code>condition: service_healthy</code>) hasta que Postgres diga \"estoy listo\" (select 1), evitando errores de inicio por condiciones de carrera (race conditions).</li> </ul>"},{"location":"02_INFRAESTRUCTURA/#23-evidencia-captura-spark-ui","title":"2.3 Evidencia: Captura Spark UI","text":"<p>Que se ve en la captura (esperado):</p> <ul> <li>URL: <code>spark://spark-master:7077</code>.</li> <li>Workers: Deber\u00eda aparecer <code>1</code> en la lista de \"Alive Workers\".</li> <li>Status: El estado del Worker debe ser <code>ALIVE</code>.</li> <li>Resources: Deber\u00eda mostrar <code>1 Cores</code> y <code>1024.0 MiB Memory</code> disponibles.</li> </ul>"},{"location":"02_INFRAESTRUCTURA/#24-prompts-utilizados-para-la-infraestructura","title":"2.4 Prompts utilizados para la infraestructura","text":""},{"location":"02_INFRAESTRUCTURA/#prompt-1-creacion-inicial","title":"Prompt 1: Creaci\u00f3n inicial","text":"<p>Herramienta: Agentic AI (Google DeepMind)</p> <p>Tu prompt exacto:</p> <pre><code>(Reconstruido del contexto de la sesi\u00f3n inicial)\nNecesito configurar un proyecto de Big Data usando Docker.\nQuiero tener un pipeline que use:\n1. PostgreSQL para guardar datos.\n2. Un cluster de Spark (Master y Worker) para procesar los datos.\n3. JupyterLab para ejecutar notebooks y conectarse a Spark.\nPor favor crea la estructura de carpetas y el archivo docker-compose.yml necesario.\n</code></pre> <p>Que te devolvio: Gener\u00f3 un archivo <code>docker-compose.yml</code> inicial con los 4 servicios solicitados, usando im\u00e1genes est\u00e1ndar (<code>postgres:13</code>, <code>bitnami/spark</code> o <code>apache/spark</code>, <code>jupyter/pyspark-notebook</code>) y defini\u00f3 una red <code>bigdata-net</code>.</p> <p>Que tuviste que cambiar y por que:</p> <ul> <li>Tuvimos problemas iniciales con las im\u00e1genes de <code>bitnami/spark</code> por temas de permisos de usuario (Running as root vs non-root).</li> <li>Cambio: Migramos a usar las im\u00e1genes oficiales <code>apache/spark:3.5.1</code> que resultaron m\u00e1s estables para este entorno.</li> <li>Ajuste: A\u00f1adimos vol\u00famenes expl\u00edcitos para compartir drivers (<code>./jars</code>) entre todos los contenedores, algo que no estaba en la primera versi\u00f3n b\u00e1sica pero fue necesario para conectar Spark con Postgres.</li> </ul>"},{"location":"02_INFRAESTRUCTURA/#prompt-2-refinamiento-de-jupyter-y-red","title":"Prompt 2: Refinamiento de Jupyter y Red","text":"<p>Herramienta: Agentic AI</p> <p>Tu prompt exacto:</p> <pre><code>(Reconstruido)\nEl contenedor de Jupyter tiene problemas de permisos para instalar librer\u00edas y no ve al Spark Master.\nAseg\u00farate de que Jupyter est\u00e9 en la misma red y configura las variables de entorno para que se conecte al master en spark://spark-master:7077.\nAdem\u00e1s, necesito persistir los notebooks en una carpeta local.\n</code></pre> <p>Que te devolvio y que cambiaste: Actualiz\u00f3 la configuraci\u00f3n de <code>jupyter-lab</code> a\u00f1adiendo <code>SPARK_DRIVER_HOST=jupyter_lab</code> (crucial para que los Workers puedan \"devolver la llamada\" al Driver) y mape\u00f3 correctamente los vol\u00famenes <code>./notebooks</code> y <code>./data</code>.</p>"},{"location":"02_INFRAESTRUCTURA/#25-recursos-web-consultados","title":"2.5 Recursos web consultados","text":"Recurso URL Que aprendiste de el Docker Hub (Postgres) https://hub.docker.com/_/postgres Variables de entorno necesarias (POSTGRES_USER, DB) Spark Documentation https://spark.apache.org/docs/latest/spark-standalone.html Puertos requeridos (8080, 7077) y configuraci\u00f3n de Master/Worker Jupyter Docker Stacks https://jupyter-docker-stacks.readthedocs.io/ C\u00f3mo configurar PySpark dentro del contenedor de Jupyter"},{"location":"02_INFRAESTRUCTURA/#26-evidencia-adicional","title":"2.6 Evidencia Adicional","text":""},{"location":"02_INFRAESTRUCTURA/#docker-desktop-contenedores-corriendo","title":"Docker Desktop: Contenedores corriendo","text":"<p>Aqu\u00ed vemos todos los servicios del <code>docker-compose.yml</code> activos y funcionando.</p> <p></p>"},{"location":"02_INFRAESTRUCTURA/#jupyter-lab-notebook-de-analisis","title":"Jupyter Lab: Notebook de An\u00e1lisis","text":"<p>Evidencia del entorno de trabajo conectado a Spark y ejecutando c\u00f3digo.</p> <p></p>"},{"location":"02_INFRAESTRUCTURA/#spark-job-ui-detalle-de-ejecucion","title":"Spark Job UI: Detalle de Ejecuci\u00f3n","text":"<p>Evidencia de los \"Jobs\" (tareas) de Spark completados correctamente durante la ejecuci\u00f3n del Random Forest.</p> <p></p>"},{"location":"02_INFRAESTRUCTURA/#27-concepto-docker-y-contenedores","title":"2.7 Concepto: Docker y Contenedores","text":"<p>\u00bfC\u00f3mo se une Docker con los Contenedores?</p> <p>Esta gr\u00e1fica explica la relaci\u00f3n entre la Imagen (la receta), el Contenedor (la tarta) y el Docker Engine (el horno).</p> <p></p> <p>Explicaci\u00f3n:</p> <ol> <li>Docker Engine: Es el software que instalaste. Act\u00faa como el intermediario entre tu sistema operativo y los contenedores.</li> <li>Im\u00e1genes: Son plantillas est\u00e1ticas (como una clase en POO o una receta de cocina). En nuestro caso, <code>apache/spark</code> es la imagen.</li> <li>Contenedores: Son la versi\u00f3n \"viva\" de la imagen. Cuando le das \"Play\", Docker Engine toma la imagen y crea un proceso aislado (el contenedor). Nota c\u00f3mo usamos la misma imagen de Spark para crear dos contenedores distintos (Master y Worker), simplemente cambi\u00e1ndoles la configuraci\u00f3n al arrancar.</li> </ol> <pre><code>\n</code></pre>"},{"location":"03_RESULTADOS/","title":"\ud83d\udcca Paso 3: Resultados y An\u00e1lisis","text":"<ul> <li>Alumno: Daniel Alexis Mendoza Corne</li> <li>Fecha: Febrero 2026</li> </ul> <p>[!IMPORTANT] Pregunta de Investigaci\u00f3n: \"\u00bfQu\u00e9 influye m\u00e1s en la riqueza de los pa\u00edses ex-sovi\u00e9ticos: tener un ej\u00e9rcito fuerte y gastar mucho en armas, o ser un pa\u00eds m\u00e1s democr\u00e1tico y con menos corrupci\u00f3n?\"</p>"},{"location":"03_RESULTADOS/#1-contexto-y-marco-teorico","title":"1. Contexto y Marco Te\u00f3rico","text":""},{"location":"03_RESULTADOS/#el-gran-juego-post-sovietico","title":"\ud83c\udf0f El \"Gran Juego\" Post-Sovi\u00e9tico","text":"<p>La regi\u00f3n analizada comprende la periferia estrat\u00e9gica de la antigua Uni\u00f3n Sovi\u00e9tica: el C\u00e1ucaso Sur (Azerbaiy\u00e1n, Georgia, Armenia) y los estados-tap\u00f3n de Asia Central y Oriental (Afganist\u00e1n, Mongolia). Tras 1991, estas naciones han transitado caminos divergentes, oscilando entre la democracia liberal y el autoritarismo, a menudo bajo la sombra de la competencia geopol\u00edtica entre potencias.</p>"},{"location":"03_RESULTADOS/#objetivo-del-estudio","title":"\ud83c\udfaf Objetivo del Estudio","text":"<p>Determinar si el desarrollo econ\u00f3mico (<code>gle_cgdpc</code>) en estas zonas de alta tensi\u00f3n geopol\u00edtica est\u00e1 impulsado principalmente por la estabilidad institucional y democr\u00e1tica (Poder Blando) o si, por el contrario, responde a din\u00e1micas de militarizaci\u00f3n y seguridad (Poder Duro).</p>"},{"location":"03_RESULTADOS/#2-metodologia-y-datos","title":"2. Metodolog\u00eda y Datos","text":""},{"location":"03_RESULTADOS/#dataset-utilizado","title":"\ud83d\udce6 Dataset Utilizado","text":"<p>Se ha utilizado el dataset est\u00e1ndar Quality of Government (QoG) de la Universidad de Gotemburgo (versi\u00f3n Enero 2026), filtrado para el periodo 1991-2023.</p>"},{"location":"03_RESULTADOS/#variables-seleccionadas","title":"\ud83d\udd0d Variables Seleccionadas","text":"<p>Para el an\u00e1lisis de Machine Learning, hemos seleccionado indicadores clave que representan nuestras dimensiones de estudio:</p> Categor\u00eda Variable Descripci\u00f3n T\u00e9cnica Hip\u00f3tesis Econom\u00eda (Target) <code>gle_cgdpc</code> PIB per c\u00e1pita real (ajustado por PPP) Variable dependiente a predecir. Poder Duro <code>wdi_expmil</code> Gasto militar (% del PIB) Refleja priorizaci\u00f3n de seguridad sobre bienestar. Poder Blando <code>p_polity2</code> \u00cdndice Polity IV (-10 a +10) Mide nivel de democracia vs autocracia. Calidad Inst. <code>vdem_corr</code> \u00cdndice de Corrupci\u00f3n V-Dem Impacto de la transparencia institucional. Social <code>wdi_lifexp</code> Esperanza de vida al nacer Indicador proxy de desarrollo humano b\u00e1sico."},{"location":"03_RESULTADOS/#3-resultados-visuales","title":"3. Resultados Visuales","text":""},{"location":"03_RESULTADOS/#31-grafico-1-matriz-de-correlacion","title":"3.1 Gr\u00e1fico 1: Matriz de Correlaci\u00f3n","text":"<p>Leyenda de Variables:</p> <ul> <li><code>gle_cgdpc</code>: PIB per c\u00e1pita (Econom\u00eda)</li> <li><code>wdi_lifexp</code>: Esperanza de Vida (Salud/Social)</li> <li><code>p_polity2</code>: \u00cdndice de Democracia (Pol\u00edtica)</li> <li><code>vdem_corr</code>: Control de la Corrupci\u00f3n (Institucional)</li> <li><code>wdi_expmil</code>: Gasto Militar (Seguridad/Geopol\u00edtica)</li> </ul> <p>[!NOTE] Interpretaci\u00f3n Al analizar la matriz, destaca el fuerte color rojo entre la esperanza de vida (<code>wdi_lifexp</code>) y el PIB, confirmando que salud y econom\u00eda van de la mano. Sin embargo, respecto a mi pregunta de investigaci\u00f3n, observo que las casillas que cruzan el Gasto Militar (<code>wdi_expmil</code>) con el PIB muestran una relaci\u00f3n compleja, a menudo desligada de la calidad democr\u00e1tica. Esto sugiere un patr\u00f3n en la regi\u00f3n donde el desarrollo econ\u00f3mico puede coexistir con altos niveles de militarizaci\u00f3n o reg\u00edmenes h\u00edbridos, validando la tensi\u00f3n entre seguridad y libertad que planteaba en mi hip\u00f3tesis.</p>"},{"location":"03_RESULTADOS/#prompt-utilizado","title":"\ud83e\udd16 Prompt Utilizado","text":"<p>[!TIP] Herramienta: Python Script (Generado v\u00eda IA)</p> <p>Tu prompt exacto:</p> <pre><code>\"Genera una matriz de correlaci\u00f3n utilizando Pandas y Seaborn para visualizar las relaciones entre las variables econ\u00f3micas (gle_cgdpc), sociales (wdi_lifexp) y pol\u00edticas (p_polity2, wdi_expmil, vdem_corr). Usa un mapa de calor (heatmap) con anotaciones num\u00e9ricas y esquema de colores 'coolwarm'.\"\n</code></pre> <p>\ud83d\udd27 Ajustes realizados: Spark maneja dataframes distribuidos incompatibles con Seaborn. Tuve que realizar una conversi\u00f3n expl\u00edcita a Pandas (<code>.toPandas()</code>) sobre una muestra de datos para poder graficar.</p>"},{"location":"03_RESULTADOS/#32-grafico-2-importancia-de-variables-random-forest","title":"3.2 Gr\u00e1fico 2: Importancia de Variables (Random Forest)","text":"<p>Leyenda de Variables:</p> <ul> <li><code>gle_cgdpc</code>: PIB per c\u00e1pita (Econom\u00eda)</li> <li><code>wdi_lifexp</code>: Esperanza de Vida (Salud/Social)</li> <li><code>p_polity2</code>: \u00cdndice de Democracia (Pol\u00edtica)</li> <li><code>vdem_corr</code>: Control de la Corrupci\u00f3n (Institucional)</li> <li><code>wdi_expmil</code>: Gasto Militar (Seguridad/Geopol\u00edtica)</li> </ul> <p>[!NOTE] Interpretaci\u00f3n Este resultado es el m\u00e1s revelador. El modelo indica que, descontando la esperanza de vida (variable de control), los factores estructurales y de seguridad (como el Gasto Militar) mantiene un peso predictivo relevante frente a las variables puramente democr\u00e1ticas (<code>p_polity2</code>). Esto responde a mi pregunta inicial: en el contexto post-sovi\u00e9tico de Asia Central, la econom\u00eda parece estar estructuralmente m\u00e1s ligada a la seguridad y la estabilidad geopol\u00edtica ('Poder Duro') que a la liberalizaci\u00f3n pol\u00edtica ('Poder Blando'). La barra de importancia nos muestra que la estabilidad del r\u00e9gimen importa m\u00e1s que su car\u00e1cter democr\u00e1tico para predecir el PIB.</p>"},{"location":"03_RESULTADOS/#prompt-utilizado_1","title":"\ud83e\udd16 Prompt Utilizado","text":"<p>[!TIP] Herramienta: Spark MLlib (Generado v\u00eda IA)</p> <p>Tu prompt exacto:</p> <pre><code>\"Entrena un modelo de regresi\u00f3n RandomForestRegressor con PySpark para predecir el PIB per c\u00e1pita. Usa VectorAssembler para combinar las features wdi_lifexp, p_polity2, vdem_corr y wdi_expmil. Despu\u00e9s del entrenamiento, extrae featureImportances y genera una gr\u00e1fica de barras.\"\n</code></pre>"},{"location":"03_RESULTADOS/#detalles-del-modelo","title":"\ud83e\udde0 Detalles del Modelo","text":"<p>Para este an\u00e1lisis se ha configurado un Random Forest Regressor en PySpark con los siguientes hiperpar\u00e1metros:</p> <ul> <li>Algoritmo: Ensamble de \u00e1rboles de decisi\u00f3n (Bagging).</li> <li>Complejidad: <code>numTrees=100</code> (100 \u00e1rboles de decisi\u00f3n en paralelo).</li> <li>Semilla: <code>seed=42</code> (Garantiza reproducibilidad de los resultados).</li> <li>Justificaci\u00f3n: Se eligi\u00f3 este algoritmo por su robustez ante valores at\u00edpicos y su capacidad para capturar relaciones no lineales complejas entre la geopol\u00edtica y la econom\u00eda, superando a modelos lineales simples. Adem\u00e1s, ofrece m\u00e9tricas nativas de Feature Importance para explicar la causalidad.</li> </ul> <p>\ud83d\udd27 Ajustes realizados: El modelo Random Forest de Spark no tolera valores nulos (<code>NaNs</code>). Implement\u00e9 una limpieza (<code>.dropna()</code>) previa al entrenamiento para evitar errores de ejecuci\u00f3n.</p>"},{"location":"03_RESULTADOS/#33-confirmacion-econometrica-test-de-hausman","title":"3.3 Confirmaci\u00f3n Econom\u00e9trica (Test de Hausman)","text":"<p>Para validar estad\u00edsticamente las relaciones inferidas por el Machine Learning, se implement\u00f3 un an\u00e1lisis de panel con dos enfoques: Efectos Fijos (FE) y Efectos Aleatorios (RE).</p> <p>[!NOTE] Resultado T\u00e9cnico El modelo de Efectos Fijos mostr\u00f3 un ajuste robusto (\\(R^2 \\approx 0.67\\)), indicando que controlar por las caracter\u00edsticas \u00fanicas e invariables de cada pa\u00eds es crucial. El modelo de Efectos Aleatorios present\u00f3 inestabilidad matem\u00e1tica, lo que refuerza la hip\u00f3tesis de que las particularidades nacionales (\"El estilo uzbeko\", \"El estilo armenio\") no son aleatorias, sino determinantes estructurales.</p>"},{"location":"03_RESULTADOS/#interpretacion-de-coeficientes-modelo-fe","title":"Interpretaci\u00f3n de Coeficientes (Modelo FE)","text":"Variable Coeficiente P-Valor Interpretaci\u00f3n Causal <code>wdi_lifexp</code> +635.55 0.000 Muy Significativo. Cada a\u00f1o extra de esperanza de vida a\u00f1ade ~$635 al PIB per c\u00e1pita. Es el motor principal. <code>p_polity2</code> +141.17 0.024 Significativo. Mejorar la democracia s\u00ed tiene un retorno econ\u00f3mico positivo directo, validando el \"Poder Blando\". <code>vdem_corr</code> -2290.3 0.019 Contraintuitivo. El modelo sugiere que aumentar el control de la corrupci\u00f3n (valores m\u00e1s altos) correlaciona negativamente con el PIB en esta muestra espec\u00edfica/periodo. Esto podr\u00eda indicar que ciertos sistemas de \"corrupci\u00f3n funcional\" o clientelismo han aceitado la econom\u00eda en etapas tempranas de transici\u00f3n. <code>wdi_expmil</code> +254.24 0.065 Marginalmente Significativo. El gasto militar impulsa la econom\u00eda (confirmando la tesis de seguridad), pero con menor certeza estad\u00edstica que la salud o la democracia."},{"location":"03_RESULTADOS/#4-discusion-y-conclusiones","title":"4. Discusi\u00f3n y Conclusiones","text":""},{"location":"03_RESULTADOS/#respuesta-a-la-pregunta-de-investigacion","title":"\ud83d\udca1 Respuesta a la Pregunta de Investigaci\u00f3n","text":"<p>[!IMPORTANT] Conclusi\u00f3n General Los datos revelan que el determinante principal del desarrollo en el 'Gran Juego' es una mezcla pragm\u00e1tica donde el Poder Duro (Seguridad) condiciona el crecimiento. Aunque la calidad de vida es esencial, mi an\u00e1lisis sugiere que estos estados priorizan la estabilidad militar/geopol\u00edtica sobre la democratizaci\u00f3n r\u00e1pida como motor econ\u00f3mico. Esto explica por qu\u00e9 naciones con democracias fr\u00e1giles pero militarmente estrat\u00e9gicas han logrado sostener ciertos niveles de desarrollo.</p>"},{"location":"03_RESULTADOS/#limitaciones-y-trabajo-futuro","title":"\u26a0\ufe0f Limitaciones y Trabajo Futuro","text":"<p>[!WARNING] Puntos a considerar:</p> <ol> <li>Datos Incompletos: Variables como el Gasto Militar (<code>wdi_expmil</code>) presentan vac\u00edos hist\u00f3ricos en pa\u00edses en conflicto (ej. Afganist\u00e1n).</li> <li>Factores Externos: El modelo ignora subsidios directos de potencias (Rusia/China) que no figuran en las m\u00e9tricas de desarrollo est\u00e1ndar.</li> <li>Complejidad del Modelo: Random Forest capta no-linealidades, pero no causalidad directa. Ser\u00eda ideal complementar con series temporales.</li> </ol>"},{"location":"04_REFLEXION_IA/","title":"Paso 4: Reflexi\u00f3n IA - \"3 Momentos Clave\"","text":"<p>Alumno: Daniel Alexis Mendoza Corne Fecha: Febrero 2026</p>"},{"location":"04_REFLEXION_IA/#bloque-a-infraestructura-docker","title":"Bloque A: Infraestructura (Docker)","text":""},{"location":"04_REFLEXION_IA/#1-arranque","title":"1. Arranque","text":"<p>\u00bfQu\u00e9 fue lo primero que le pediste a la IA? Le ped\u00ed ayuda para crear el archivo <code>docker-compose.yml</code>. No ten\u00eda muy claro c\u00f3mo conectar Spark (el Master y el Worker) con JupyterLab y la base de datos Postgres, as\u00ed que le ped\u00ed que me generara la estructura b\u00e1sica para que todo funcionara junto.</p>"},{"location":"04_REFLEXION_IA/#2-error","title":"2. Error","text":"<p>\u00bfQu\u00e9 fall\u00f3 y c\u00f3mo lo resolviste? Al principio no pod\u00eda entrar a los servicios. Intentaba poner en el navegador los puertos que ve\u00eda en el archivo, como el <code>7077</code> o el <code>5432</code>, y me sal\u00eda error de p\u00e1gina no encontrada.</p> <ul> <li>Resoluci\u00f3n: La IA me explic\u00f3 que esos puertos son internos para que se hablen las m\u00e1quinas entre ellas. Para yo ver algo, ten\u00eda que usar los puertos \"web\" o visuales, que eran el <code>8080</code> para ver Spark y el <code>8888</code> para mi Jupyter. \u00a1Vaya l\u00edo de puertos!</li> </ul> <p>Otro problema: PySpark no aparec\u00eda</p> <ul> <li>Fallo: Cuando corr\u00eda mi c\u00f3digo, me dec\u00eda <code>ModuleNotFoundError: No module named 'pyspark'</code>.</li> <li>Soluci\u00f3n: Resulta que aunque la imagen de Docker ten\u00eda Spark, mi script de Python no lo encontraba. Tuve que a\u00f1adir <code>pyspark==3.5.0</code> al archivo <code>requirements.txt</code> y volver a construir la imagen.</li> </ul>"},{"location":"04_REFLEXION_IA/#3-aprendizaje","title":"3. Aprendizaje","text":"<p>\u00bfQu\u00e9 aprendiste que NO sab\u00edas antes? Entend\u00ed la diferencia entre los puertos que \"expongo\" hacia afuera (para m\u00ed) y los que se quedan dentro de la red de Docker. Tambi\u00e9n aprend\u00ed a usar <code>volumes</code> para guardar mis notebooks, porque la primera vez reinici\u00e9 el contenedor y... \u00a1adi\u00f3s trabajo!</p> <p>Otro Error Detectado: Spark Worker Offline</p> <ul> <li>Fallo: En la interfaz <code>localhost:8080</code>, aparec\u00eda \"Alive Workers: 0\" aunque el contenedor exist\u00eda.</li> <li>Causa: Al reconstruir y levantar solo el servicio <code>jupyter-lab</code>, docker-compose no necesariamente reinicia o mantiene activos los contenedores dependientes si no se especifican.</li> <li>Resoluci\u00f3n: Ejecutar <code>docker-compose up -d</code> (sin especificar servicio) y verificar con <code>docker ps</code> asegur\u00f3 que tanto Master como Worker estuvieran activos.</li> <li>Aprendizaje: La \"Arquitectura Distribuida\" requiere validaci\u00f3n expl\u00edcita de que todos los nodos est\u00e1n vivos, no basta con que el c\u00f3digo corra (que puede estar en modo local).</li> </ul>"},{"location":"04_REFLEXION_IA/#prompt-clave-bloque-a","title":"\ud83d\udcac Prompt Clave (Bloque A)","text":"<pre><code>\"Genera la configuraci\u00f3n de docker-compose.yml para un entorno Spark clusterizado (1 Master, 1 Worker) con persistencia de datos en PostgreSQL. Aseg\u00farate de exponer los puertos UI (8080, 4040) y configurar la red bridge para que JupyterLab pueda acceder al Spark Master mediante el nombre del servicio.\"\n</code></pre>"},{"location":"04_REFLEXION_IA/#bloque-b-pipeline-etl-spark","title":"Bloque B: Pipeline ETL (Spark)","text":""},{"location":"04_REFLEXION_IA/#1-arranque_1","title":"1. Arranque","text":"<p>\u00bfQu\u00e9 fue lo primero que le pediste a la IA? Necesitaba ayuda para hacer el script <code>pipeline.py</code>. Quer\u00eda que leyera el dataset QoG pero que solo se quedara con los 5 pa\u00edses que me interesaban del \"Gran Juego\" y que adem\u00e1s me creara una columna nueva para agruparlos por zona.</p>"},{"location":"04_REFLEXION_IA/#2-error_1","title":"2. Error","text":"<p>\u00bfQu\u00e9 fall\u00f3 y c\u00f3mo lo resolviste? Tuve problemas graves al intentar subir todo a GitHub. Error: <code>fatal: not a git repository</code>.</p> <ul> <li>Resoluci\u00f3n: Me hab\u00eda olvidado de iniciar el repositorio con <code>git init</code>. La IA me guio paso a paso: iniciar git, configurar el <code>.gitignore</code> (s\u00faper importante para no subir datos pesados por error) y luego vincularlo con mi repo en GitHub.</li> </ul>"},{"location":"04_REFLEXION_IA/#3-aprendizaje_1","title":"3. Aprendizaje","text":"<p>\u00bfQu\u00e9 aprendiste que NO sab\u00edas antes? Aprend\u00ed a usar <code>pyspark.sql.functions.when</code>. Antes hac\u00eda esto con bucles <code>for</code> en Python normal, pero con Big Data eso es lent\u00edsimo. Con esta funci\u00f3n de Spark, puedo crear columnas condicionales (como la de <code>subregion</code>) de forma s\u00faper r\u00e1pida y distribuida.</p>"},{"location":"04_REFLEXION_IA/#prompt-clave-bloque-b","title":"\ud83d\udcac Prompt Clave (Bloque B)","text":"<pre><code>\"Quiero subir mi proyecto a GitHub pero evitar errores. Dame los pasos exactos para iniciar el repositorio, crear un archivo `.gitignore` que excluya mis datos pesados (carpeta /data) y los archivos temporales de Jupyter, y finalmente c\u00f3mo conectar mi carpeta local con el repositorio remoto main.\"\n</code></pre>"},{"location":"04_REFLEXION_IA/#bloque-c-analisis-de-datos-machine-learning","title":"Bloque C: An\u00e1lisis de Datos (Machine Learning)","text":""},{"location":"04_REFLEXION_IA/#1-arranque_2","title":"1. Arranque","text":"<p>\u00bfQu\u00e9 fue lo primero que le pediste a la IA? Le pregunt\u00e9 qu\u00e9 modelo de Machine Learning me conven\u00eda m\u00e1s. Estaba dudando entre KNN, SVM o Random Forest para ver c\u00f3mo influ\u00edan las variables pol\u00edticas en la econom\u00eda.</p>"},{"location":"04_REFLEXION_IA/#2-error_2","title":"2. Error","text":"<p>\u00bfQu\u00e9 fall\u00f3 y c\u00f3mo lo resolviste? Quise generar las gr\u00e1ficas autom\u00e1ticamente corriendo el notebook desde la terminal, pero todo explot\u00f3. Error: <code>TypeError: 'JavaPackage' object is not callable</code>.</p> <ul> <li>Resoluci\u00f3n: La IA me sugiri\u00f3 que no mezclara cosas. En lugar de forzar el notebook, pas\u00e9 la l\u00f3gica a un script limpio en Python (<code>src/analysis.py</code>) y lo ejecut\u00e9 con <code>spark-submit</code>. Funcion\u00f3 mucho mejor y sin conflictos raros de Java.</li> </ul>"},{"location":"04_REFLEXION_IA/#3-aprendizaje_2","title":"3. Aprendizaje","text":"<p>\u00bfQu\u00e9 aprendiste que NO sab\u00edas antes? Que Random Forest es genial no solo para predecir, sino para explicar por qu\u00e9 predice lo que predice (feature importance). Eso me ayud\u00f3 mucho m\u00e1s que KNN para entender mi problema de investigaci\u00f3n. Tambi\u00e9n aprend\u00ed que automatizar gr\u00e1ficas es mejor que hacerlas a mano una por una.</p>"},{"location":"04_REFLEXION_IA/#prompt-clave-bloque-c","title":"\ud83d\udcac Prompt Clave (Bloque C)","text":"<pre><code>\"Act\u00faa como un experto en Data Science. Tengo un dataset con variables sociopol\u00edticas y quiero predecir el impacto en el PIB. Eval\u00faa comparativamente KNN, SVM y Random Forest justificando cu\u00e1l es m\u00e1s adecuado considerando la explicabilidad (feature importance), manejo de outliers y la dimensionalidad de mis datos.\"\n</code></pre>"},{"location":"05_EXPLICACION_CODIGO/","title":"\ud83d\udcd8Paso 5: Documentaci\u00f3n T\u00e9cnica del C\u00f3digo Fuente","text":"<p>Proyecto: Big Data &amp; Geopol\u00edtica (\"El Gran Juego\") Alumno: Daniel Alexis Mendoza Corne Fecha: Febrero 2026</p>"},{"location":"05_EXPLICACION_CODIGO/#1-por-que-la-carpeta-se-llama-src","title":"1. \u00bfPor qu\u00e9 la carpeta se llama <code>src</code>?","text":"<p><code>src</code> es la abreviatura est\u00e1ndar en ingenier\u00eda de software para \"Source\" (C\u00f3digo Fuente).</p> <p>En proyectos profesionales, es fundamental mantener separado el c\u00f3digo l\u00f3gico de otros elementos. Esta estructura garantiza:</p> <ul> <li>Orden: El c\u00f3digo no se mezcla con la documentaci\u00f3n (<code>.md</code>), la configuraci\u00f3n (<code>docker/</code>) o los datos (<code>data/</code>).</li> <li>Seguridad: Facilita la configuraci\u00f3n de permisos; por ejemplo, el servidor de producci\u00f3n solo necesita acceso de lectura a <code>src</code>, pero de escritura a <code>data</code>.</li> <li>Escalabilidad: Si el proyecto crece, todo el c\u00f3digo l\u00f3gica reside en un \u00fanico punto de verdad.</li> </ul>"},{"location":"05_EXPLICACION_CODIGO/#2-catalogo-de-scripts","title":"2. Cat\u00e1logo de Scripts","text":"<p>A continuaci\u00f3n, se detalla la funci\u00f3n t\u00e9cnica y de negocio de cada m\u00f3dulo desarrollado.</p>"},{"location":"05_EXPLICACION_CODIGO/#1-infraestructura-y-preparacion","title":"\ud83d\udee0\ufe0f 1. Infraestructura y Preparaci\u00f3n","text":""},{"location":"05_EXPLICACION_CODIGO/#download_datapy","title":"<code>download_data.py</code>","text":"<ul> <li>Funci\u00f3n: Automatizaci\u00f3n de Ingesta.</li> <li>Qu\u00e9 hace: Se conecta al repositorio de la Universidad de Gotemburgo, descarga el dataset <code>.csv</code> de 68MB y lo coloca en la ruta <code>data/raw/</code>.</li> <li>Por qu\u00e9 es importante: Elimina la dependencia de descargas manuales, haciendo que el proyecto sea reproducible en cualquier m\u00e1quina con un solo comando.</li> </ul>"},{"location":"05_EXPLICACION_CODIGO/#verify_sparkpy","title":"<code>verify_spark.py</code>","text":"<ul> <li>Funci\u00f3n: Test de Integridad (Smoke Test).</li> <li>Qu\u00e9 hace: Intenta iniciar una sesi\u00f3n de Spark y crear un DataFrame peque\u00f1o en memoria.</li> <li>Por qu\u00e9 es importante: Es el primer script que ejecutamos para validar que el contenedor de Docker y el cluster de Spark est\u00e1n comunic\u00e1ndose correctamente antes de lanzar procesos pesados.</li> </ul>"},{"location":"05_EXPLICACION_CODIGO/#2-procesamiento-de-datos-etl","title":"\u2699\ufe0f 2. Procesamiento de Datos (ETL)","text":""},{"location":"05_EXPLICACION_CODIGO/#pipelinepy","title":"<code>pipeline.py</code>","text":"<ul> <li>Funci\u00f3n: ETL (Extract, Transform, Load).</li> <li>Tecnolog\u00eda: Apache Spark (PySpark SQL).</li> <li>Flujo de Trabajo:</li> <li>Extract: Lee el CSV crudo.</li> <li>Transform:<ul> <li>Filtra los 5 pa\u00edses del \"Gran Juego\" (Afganist\u00e1n, Mongolia, C\u00e1ucaso).</li> <li>Crea la variable derivada <code>subregion</code>.</li> <li>Castea tipos de datos (Strings a Doubles) para asegurar precisi\u00f3n matem\u00e1tica.</li> </ul> </li> <li>Load: Guarda el resultado limpio en formato Parquet.</li> <li>Detalle Pro: Usamos <code>.parquet</code> en lugar de <code>.csv</code> porque es un formato columnar comprimido que es mucho m\u00e1s r\u00e1pido para leer en an\u00e1lisis posteriores de Big Data.</li> </ul>"},{"location":"05_EXPLICACION_CODIGO/#ingest_datapy-modulo-legado","title":"<code>ingest_data.py</code> (M\u00f3dulo Legado)","text":"<ul> <li>Funci\u00f3n: Conector a Base de Datos Relacional.</li> <li>Qu\u00e9 hace: Estaba dise\u00f1ado para cargar los datos en PostgreSQL.</li> <li>Estado: Se mantiene como respaldo. Para el an\u00e1lisis principal optamos por el flujo Spark-Parquet por ser m\u00e1s nativo del ecosistema de Big Data que el almacenamiento SQL tradicional.</li> </ul>"},{"location":"05_EXPLICACION_CODIGO/#3-analisis-avanzado-y-resultados","title":"\ud83e\udde0 3. An\u00e1lisis Avanzado y Resultados","text":""},{"location":"05_EXPLICACION_CODIGO/#analysispy","title":"<code>analysis.py</code>","text":"<ul> <li>Funci\u00f3n: Motor de Machine Learning.</li> <li>Tecnolog\u00eda: Spark MLlib.</li> <li>Qu\u00e9 hace:</li> <li>Carga los datos procesados (Parquet).</li> <li>Matriz de Correlaci\u00f3n: Calcula c\u00f3mo se relacionan las variables (ej. Gasto Militar vs PIB).</li> <li>Random Forest: Entrena un modelo de Inteligencia Artificial compuesto por 100 \u00e1rboles de decisi\u00f3n para predecir el desarrollo econ\u00f3mico.</li> <li>Feature Importance: Extrae qu\u00e9 variables tuvieron m\u00e1s peso en la decisi\u00f3n del modelo.</li> <li>Salida: Genera autom\u00e1ticamente los gr\u00e1ficos est\u00e1ticos <code>.png</code> en la carpeta <code>notebooks/</code>.</li> </ul>"},{"location":"05_EXPLICACION_CODIGO/#econometric_analysispy","title":"<code>econometric_analysis.py</code>","text":"<ul> <li>Funci\u00f3n: An\u00e1lisis Econom\u00e9trico Riguroso.</li> <li>Tecnolog\u00eda: Librer\u00eda <code>linearmodels</code> (Python).</li> <li>Qu\u00e9 hace:</li> <li>Ejecuta modelos de regresi\u00f3n para datos de panel: Efectos Fijos (Fixed Effects) y Efectos Aleatorios (Random Effects).</li> <li>Implementa el Test de Hausman para determinar cu\u00e1l de los dos modelos es estad\u00edsticamente m\u00e1s adecuado (causalidad vs correlaci\u00f3n).</li> <li>Genera un reporte detallado en <code>notebooks/hausman_results.txt</code>.</li> <li>Valor agregado: Complementa la \"caja negra\" del Machine Learning (Random Forest) con inferencia estad\u00edstica cl\u00e1sica, validando si las caracter\u00edsticas \u00fanicas de cada pa\u00eds sesgan los resultados.</li> </ul>"},{"location":"05_EXPLICACION_CODIGO/#4-interfaz-de-usuario-frontend","title":"\ud83d\ude80 4. Interfaz de Usuario (Frontend)","text":""},{"location":"05_EXPLICACION_CODIGO/#srcapp_streamlitpy-y-srcapp_streamlit_propy","title":"<code>src/app_streamlit.py</code> y <code>src/app_streamlit_pro.py</code>","text":"<p>Son el Frontend de la aplicaci\u00f3n.</p> <ul> <li>Tecnolog\u00eda: Streamlit.</li> <li>Funciones:</li> <li>Cargar el Parquet procesado.</li> <li>Generar gr\u00e1ficos interactivos con Plotly.</li> <li>Pro Version: Incluye globo 3D, radar charts y est\u00e9tica \"Dark Mode\".</li> <li>Sirve una interfaz web en el puerto <code>8501</code>.</li> <li>Permite al usuario explorar los datos: filtrar por a\u00f1o, ver tendencias temporales interactivas y simular predicciones.</li> <li>Es la \"cara\" del proyecto, transformando el c\u00f3digo t\u00e9cnico en un producto visual consumible por un usuario final.</li> </ul>"},{"location":"05_EXPLICACION_CODIGO/#3-diagrama-de-flujo-de-datos","title":"3. Diagrama de Flujo de Datos","text":"<pre><code>graph TD\n    %% Estilos\n    classDef source fill:#f9f,stroke:#333,stroke-width:2px;\n    classDef script fill:#bbf,stroke:#333,stroke-width:2px,color:black;\n    classDef data fill:#dfd,stroke:#333,stroke-width:2px,color:black;\n    classDef output fill:#fd9,stroke:#333,stroke-width:2px,color:black,stroke-dasharray: 5 5;\n\n    subgraph INGESTA [\"\ud83d\udce1 Ingesta de Datos\"]\n        A[\"\u2601\ufe0f Internet / Repo QoG\"]:::source\n        Script1{{\"\ud83d\udc0d download_data.py\"}}:::script\n    end\n\n    subgraph PROCESAMIENTO [\"\u2699\ufe0f Procesamiento &amp; An\u00e1lisis\"]\n        Script2{{\"\u26a1 pipeline.py\"}}:::script\n        Script3{{\"\ud83e\udde0 analysis.py\"}}:::script\n        Script5{{\"\ud83d\udcc9 econometric_analysis.py\"}}:::script\n    end\n\n    subgraph ALMACENAMIENTO [\"\ud83d\udcbe Almacenamiento\"]\n        B[(\"\ud83d\udcc4 Raw CSV\")]:::data\n        C[(\"\ud83d\udce6 Clean Parquet\")]:::data\n    end\n\n    subgraph VISUALIZACION [\"\ud83d\udcca Consumo &amp; UI\"]\n        Script4{{\"\ud83d\ude80 app_streamlit_pro.py\"}}:::script\n        D[\"\ud83d\udcc8 Gr\u00e1ficos Est\u00e1ticos .png\"]:::output\n        E[\"\ud83d\udda5\ufe0f Dashboard 3D Interactivo\"]:::output\n        F[\"\ud83d\udcc4 Reporte Hausman .txt\"]:::output\n    end\n\n    %% Relaciones\n    A --&gt; Script1\n    Script1 --&gt; B\n    B --&gt; Script2\n    Script2 --&gt; C\n    C --&gt; Script3\n    C --&gt; Script4\n    C --&gt; Script5\n    Script3 --&gt; D\n    Script4 --&gt; E\n    Script5 --&gt; F</code></pre> <p>[!NOTE] Conclusi\u00f3n del Flujo de Datos: Como se observa en el diagrama, el proyecto sigue una arquitectura lineal de Big Data moderna:</p> <ol> <li>Ingesta: Los datos se capturan autom\u00e1ticamente de internet (<code>download_data.py</code>).</li> <li>Procesamiento: Se limpian y estructuran en Spark (<code>pipeline.py</code>), guard\u00e1ndose en formato eficiente Parquet.</li> <li>Consumo: A partir del dato limpio, se derivan tres productos finales: An\u00e1lisis ML (<code>analysis.py</code>), Validaci\u00f3n Estad\u00edstica (<code>econometric_analysis.py</code>) y Visualizaci\u00f3n Interactiva (<code>app_streamlit_pro.py</code>).</li> </ol> <p>Esta estructura modular asegura que si cambiamos la fuente de datos, solo tocamos el script de Ingesta, sin romper el Dashboard final.</p>"},{"location":"05_EXPLICACION_CODIGO/#4-devops-y-documentacion","title":"4. DevOps y Documentaci\u00f3n \ud83d\udcda","text":"<p>Para desplegar este sitio web, utilizamos dos archivos clave que a menudo se confunden pero tienen prop\u00f3sitos muy distintos:</p>"},{"location":"05_EXPLICACION_CODIGO/#mkdocsyml-el-cerebro","title":"<code>mkdocs.yml</code> (El Cerebro \ud83e\udde0)","text":"<p>Ubicaci\u00f3n: Ra\u00edz del proyecto. Funci\u00f3n: Configuraci\u00f3n del Sitio Web. Qu\u00e9 hace:</p> <ul> <li>Define el t\u00edtulo del sitio, el autor y el tema visual (\"Material\").</li> <li>Estructura el men\u00fa de navegaci\u00f3n lateral.</li> <li>Activa plugins y extensiones (como Mermaid para los gr\u00e1ficos).</li> <li>Es el archivo que t\u00fa editas cuando quieres cambiar el contenido, el orden de las p\u00e1ginas o el color del sitio.</li> </ul>"},{"location":"05_EXPLICACION_CODIGO/#githubworkflowsdeploy_docsyml-el-obrero","title":"<code>.github/workflows/deploy_docs.yml</code> (El Obrero \ud83d\udc77)","text":"<p>Ubicaci\u00f3n: <code>.github/workflows/</code> (antes llamado <code>mkdocs.yml</code>). Funci\u00f3n: Automatizaci\u00f3n del Despliegue (CI/CD). Qu\u00e9 hace:</p> <ul> <li>Es un script de instrucciones para los servidores de GitHub (GitHub Actions).</li> <li>Cada vez que haces un cambio (<code>git push</code>), este archivo le dice a GitHub:</li> <li>\"Instala Python y MkDocs\".</li> <li>\"Instala los plugins necesarios (Material, Mermaid)\".</li> <li>\"Construye la p\u00e1gina web est\u00e1tica\".</li> <li>\"Publicala en internet (GitHub Pages)\".</li> <li>No necesitas editarlo casi nunca, salvo que cambies la forma de desplegar el sitio.</li> </ul>"},{"location":"06_RESPUESTAS/","title":"Paso 6: Preguntas de Comprensi\u00f3n","text":"<p>Alumno: Daniel Alexis Mendoza Corne</p> <p>Instrucciones: Responde cada pregunta con tus propias palabras. Las respuestas deben ser espec\u00edficas y demostrar que entiendes los conceptos. Se acepta entre 3-5 oraciones por pregunta.</p> <p>Nota: Completa este archivo AL FINAL, despu\u00e9s de haber terminado los bloques A, B y C. As\u00ed tendr\u00e1s la experiencia necesaria para responder.</p>"},{"location":"06_RESPUESTAS/#1-infraestructura","title":"1. Infraestructura","text":"<p>Si tu worker tiene 2 GB de RAM y el CSV pesa 3 GB, \u00bfqu\u00e9 pasa? \u00bfC\u00f3mo lo solucionar\u00edas?</p> <p>Si intento cargar todo el archivo de golpe (como hace Pandas), me dar\u00eda un error de memoria (<code>Out Of Memory</code>) porque 3GB no entran en 2GB. Pero Spark es inteligente: no carga todo, sino que divide el archivo en \"pedacitos\" (particiones) y los procesa uno por uno. Si aun as\u00ed fallara, la soluci\u00f3n ser\u00eda aumentar el n\u00famero de particiones con <code>.repartition()</code> para que cada trozo sea m\u00e1s peque\u00f1o, o a\u00f1adir m\u00e1s nodos Worker al cluster para repartir la carga.</p>"},{"location":"06_RESPUESTAS/#2-etl","title":"2. ETL","text":"<p>\u00bfPor qu\u00e9 <code>spark.read.csv()</code> no ejecuta nada hasta que llamas <code>.count()</code> o <code>.show()</code>?</p> <p>Porque Spark es \"perezoso\" (Lazy Evaluation). Cuando le digo \"lee esto\" o \"filtra aquello\", en realidad no lo hace al momento, solo se apunta la tarea en un plan (DAG). Solo se pone a trabajar de verdad cuando le pido un resultado final (una acci\u00f3n como <code>count</code> o <code>show</code>). Esto es genial porque si filtro datos, Spark se da cuenta y ni siquiera lee lo que no necesito, ahorrando tiempo.</p>"},{"location":"06_RESPUESTAS/#3-analisis","title":"3. An\u00e1lisis","text":"<p>Interpreta tu gr\u00e1fico principal: \u00bfqu\u00e9 patr\u00f3n ves y por qu\u00e9 crees que ocurre?</p> <p>Mirando el gr\u00e1fico de importancia de variables (Random Forest), veo que el Gasto Militar influye mucho m\u00e1s que la democracia. Esto tiene sentido para mi zona de estudio (\"El Gran Juego\"): parece que los pa\u00edses que m\u00e1s crecen econ\u00f3micamente (como Azerbaiy\u00e1n) no son los m\u00e1s democr\u00e1ticos, sino los que tienen un estado fuerte y militarizado para mantener el control. Vamos, que la estabilidad pol\u00edtica pesa m\u00e1s que la libertad en esta regi\u00f3n.</p> <p></p> <p>Leyenda de Variables:</p> <ul> <li><code>wdi_lifexp</code>: Esperanza de Vida (Salud/Social)</li> <li><code>wdi_expmil</code>: Gasto Militar (Poder Duro)</li> <li><code>vdem_corr</code>: Control de Corrupci\u00f3n (Institucional)</li> <li><code>p_polity2</code>: \u00cdndice de Democracia (Poder Blando)</li> </ul>"},{"location":"06_RESPUESTAS/#4-escalabilidad","title":"4. Escalabilidad","text":"<p>Si tuvieras que repetir este ejercicio con un dataset de 50 GB, \u00bfqu\u00e9 cambiar\u00edas en tu infraestructura?</p> <p>\u00a1Mi port\u00e1til explotar\u00eda! Docker Desktop no aguantar\u00eda eso. Tendr\u00eda que llevarme el proyecto a la nube (como AWS o Databricks) y usar un sistema de almacenamiento distribuido de verdad (HDFS o S3) en vez de mi disco duro. Adem\u00e1s, necesitar\u00eda un cluster con varios Workers (m\u00e1quinas conectadas), porque un solo nodo no podr\u00eda con tanto volumen de datos en un tiempo razonable.</p>"},{"location":"07_PROTOTIPO/","title":"\ud83c\udfa5 Paso 7: Prototipo / Demo del Dashboard","text":"<p>Aqu\u00ed puedes ver una demostraci\u00f3n r\u00e1pida de la funcionalidad del Dashboard \"El Gran Juego\".</p>"},{"location":"07_PROTOTIPO/#_1","title":"\ud83c\udfa5 Paso 7: Prototipo / Demo del Dashboard","text":"<p>Nota: En el Dashboard interactivo, esta imagen se sustituye autom\u00e1ticamente por el reproductor de video.</p>"}]}